{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo-list\n",
    "=========\n",
    "<li>Migrate static->stream(if possible)</li>\n",
    "<li>Find new efficient way to do pattern detection/anomaly detection</li>\n",
    "<li>Email alert config (now sent to yanisa.sunthornyotin@cern.ch)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "userSchema = StructType() \\\n",
    "        .add(\"window\",StructType()\\\n",
    "             .add(\"start\",TimestampType())\\\n",
    "             .add(\"end\",TimestampType()))\\\n",
    "        .add(\"system\", StringType())\\\n",
    "        .add(\"count\", LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- system: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data = spark\\\n",
    ".readStream.format(\"parquet\")\\\n",
    ".schema(userSchema)\\\n",
    ".load(\"/cms/users/carizapo/ming/groupdata_cmsweb_logs\");\n",
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- system: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_data = spark\\\n",
    ".read.format(\"parquet\")\\\n",
    ".load(\"/cms/users/carizapo/ming/groupdata_cmsweb_logs\");\n",
    "temp_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|              window|              system|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|[2019-06-21 10:02...|/das/request?pid=...|    4|\n",
      "|[2019-06-21 09:29...|/das/request?pid=...|    3|\n",
      "|[2019-06-21 09:44...|/das/request?pid=...|    9|\n",
      "|[2019-06-21 09:44...|/das/request?pid=...|    6|\n",
      "|[2019-06-21 09:43...|/das/request?pid=...|    1|\n",
      "|[2019-06-21 10:02...|/das/request?inst...|    1|\n",
      "|[2019-06-21 09:28...|/das/request?inst...|    1|\n",
      "|[2019-06-21 09:44...|/das/request?inst...|    1|\n",
      "|[2019-06-21 09:44...|/das/request?inpu...|    1|\n",
      "|[2019-06-21 09:43...|/das/request?inst...|    1|\n",
      "|[2019-06-21 09:53...|/das/request?pid=...|    1|\n",
      "|[2019-06-21 09:23...|/das/request?pid=...|    2|\n",
      "|[2019-06-21 09:53...|/das/request?view...|    1|\n",
      "|[2019-06-21 09:23...|/das/request?view...|    1|\n",
      "|[2019-06-21 09:38...|/dqm/offline/data...|    1|\n",
      "|[2019-06-21 09:38...|/dqm/offline/data...|    1|\n",
      "|[2019-06-21 09:42...|/dqm/offline/data...|    1|\n",
      "|[2019-06-21 09:42...|/dqm/offline/data...|    1|\n",
      "|[2019-06-21 09:42...|/dqm/offline/data...|    1|\n",
      "|[2019-06-21 09:44...|/dqm/offline/data...|    1|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window.partitionBy('system',window(\"window.start\", \"7 days\"))\n",
    "# .orderBy(col(\"window.start\").cast('long')).rangeBetween(-days(7), 0)\n",
    "\n",
    "freq_analyze_df=temp_data.select('*', avg('count').over(w).alias('avg')).sort('system','window')\\\n",
    ".select('*', (col('count') - first('avg').over(w)).alias('diff'))\\\n",
    ".select('*', when((abs(col('diff')) > col('avg')*0.7), 1).otherwise(0).alias('label'))\n",
    "# freq_analyze_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- system: string (nullable = true)\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- avg: double (nullable = true)\n",
      " |-- diff: double (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data=raw_data.join(freq_analyze_df, [\"system\",\"window\",\"count\"], \"inner\")\n",
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_alert_data = raw_data.filter(\"label > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_flow = raw_data.writeStream.queryName(\"hdfs\").outputMode(\"Append\").format(\"memory\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_alert_data_flow = filter_alert_data.writeStream.queryName(\"alert\").outputMode(\"Append\").format(\"memory\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_flow.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_alert_data_flow.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_data = raw_data.withColumn('feature', concat(col('system'), col('count'))).writeStream.queryName(\"concat\").outputMode(\"Append\").format(\"memory\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_data.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7f04fb5fee48>,\n",
       " <pyspark.sql.streaming.StreamingQuery at 0x7f04fb5fecf8>,\n",
       " <pyspark.sql.streaming.StreamingQuery at 0x7f04fb5fe470>,\n",
       " <pyspark.sql.streaming.StreamingQuery at 0x7f04fb5fe5f8>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_flow.lastProgress\n",
    "filter_alert_data_flow.lastProgress\n",
    "# raw_data_flow.processAllAvailable()\n",
    "# filter_alert_data_flow.processAllAvailable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----+----+-----+-----+\n",
      "|system|              window|count| avg| diff|label|\n",
      "+------+--------------------+-----+----+-----+-----+\n",
      "|   dqm|[2019-06-21 09:57...|    1|21.5|-20.5|    1|\n",
      "|   dqm|[2019-06-21 09:58...|    1|21.5|-20.5|    1|\n",
      "|   dqm|[2019-06-21 09:59...|   24|21.5|  2.5|    0|\n",
      "|   dqm|[2019-06-21 10:00...|   19|21.5| -2.5|    0|\n",
      "|   dqm|[2019-06-21 09:24...|    4|21.5|-17.5|    1|\n",
      "|   dqm|[2019-06-21 09:27...|   12|21.5| -9.5|    0|\n",
      "|   dqm|[2019-06-21 09:28...|   30|21.5|  8.5|    0|\n",
      "|   dqm|[2019-06-21 09:29...|   39|21.5| 17.5|    1|\n",
      "|   dqm|[2019-06-21 09:30...|   14|21.5| -7.5|    0|\n",
      "|   dqm|[2019-06-21 09:38...|   26|21.5|  4.5|    0|\n",
      "|   dqm|[2019-06-21 09:39...|   10|21.5|-11.5|    0|\n",
      "|   dqm|[2019-06-21 09:42...|   22|21.5|  0.5|    0|\n",
      "|   dqm|[2019-06-21 09:43...|   34|21.5| 12.5|    0|\n",
      "|   dqm|[2019-06-21 09:44...|   65|21.5| 43.5|    1|\n",
      "|   dqm|[2019-06-21 09:45...|    5|21.5|-16.5|    1|\n",
      "|   dqm|[2019-06-21 09:53...|   29|21.5|  7.5|    0|\n",
      "|   dqm|[2019-06-21 09:23...|   18|21.5| -3.5|    0|\n",
      "|   dqm|[2019-06-21 10:01...|   18|21.5| -3.5|    0|\n",
      "|   dqm|[2019-06-21 10:02...|   57|21.5| 35.5|    1|\n",
      "|   dqm|[2019-06-21 10:03...|    2|21.5|-19.5|    1|\n",
      "+------+--------------------+-----+----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alerts = spark.sql(\"select * from hdfs\")\n",
    "alerts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----+------+-------+-----+\n",
      "|        system|              window|count|   avg|   diff|label|\n",
      "+--------------+--------------------+-----+------+-------+-----+\n",
      "|           dqm|[2019-06-21 09:24...|    4|  21.5|  -17.5|    1|\n",
      "|           dqm|[2019-06-21 09:29...|   39|  21.5|   17.5|    1|\n",
      "|           dqm|[2019-06-21 09:44...|   65|  21.5|   43.5|    1|\n",
      "|           dqm|[2019-06-21 09:45...|    5|  21.5|  -16.5|    1|\n",
      "|           dqm|[2019-06-21 09:57...|    1|  21.5|  -20.5|    1|\n",
      "|           dqm|[2019-06-21 09:58...|    1|  21.5|  -20.5|    1|\n",
      "|           dqm|[2019-06-21 10:02...|   57|  21.5|   35.5|    1|\n",
      "|           dqm|[2019-06-21 10:03...|    2|  21.5|  -19.5|    1|\n",
      "|           dbs|[2019-06-21 10:02...| 6735|1428.2| 5306.8|    1|\n",
      "|           dbs|[2019-06-21 10:03...|  273|1428.2|-1155.2|    1|\n",
      "|           dbs|[2019-06-21 09:58...|  362|1428.2|-1066.2|    1|\n",
      "|           dbs|[2019-06-21 09:59...| 4514|1428.2| 3085.8|    1|\n",
      "|           dbs|[2019-06-21 10:00...|  339|1428.2|-1089.2|    1|\n",
      "|           dbs|[2019-06-21 09:39...|  152|1428.2|-1276.2|    1|\n",
      "|           dbs|[2019-06-21 09:42...|  279|1428.2|-1149.2|    1|\n",
      "|           dbs|[2019-06-21 09:45...|  263|1428.2|-1165.2|    1|\n",
      "|           dbs|[2019-06-21 09:53...| 2993|1428.2| 1564.8|    1|\n",
      "|           dbs|[2019-06-21 09:57...|   63|1428.2|-1365.2|    1|\n",
      "|           dbs|[2019-06-21 09:24...|   14|1428.2|-1414.2|    1|\n",
      "|/server-status|[2019-06-21 09:29...|   32|  18.0|   14.0|    1|\n",
      "+--------------+--------------------+-----+------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alerts = spark.sql(\"select * from alert\")\n",
    "alerts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notifier import Notifier\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "notifier = Notifier(config=json.loads(s='''\n",
    "{\n",
    "  \"cases\": {\n",
    "    \"exit_2\": {\n",
    "      \"alert_name\": \"cms-htcondor-es-validation\",\n",
    "      \"email\": {\n",
    "        \"send_ok\": true,\n",
    "        \"to\": [\n",
    "          \"yanisa.sunthornyotin@cern.ch\"\n",
    "        ]\n",
    "      },\n",
    "      \"entities\": [\n",
    "        \"default entity\"\n",
    "      ],\n",
    "      \"snow\": {\n",
    "        \"assignment_level\": 3,\n",
    "        \"functional_element\": \"\",\n",
    "        \"grouping\": true,\n",
    "        \"service_element\": \"MONITORING\"\n",
    "      },\n",
    "      \"source\": \"cms-monit-notifier\",\n",
    "      \"status\": \"ERROR\",\n",
    "      \"targets\": [\n",
    "        \"email\",\n",
    "        \"snow\"\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"default_case\": {\n",
    "    \"alert_name\": \"cms-htcondor-es-validation\",\n",
    "    \"email\": {\n",
    "      \"send_ok\": true,\n",
    "      \"to\": [\n",
    "        \"yanisa.sunthornyotin@cern.ch\"\n",
    "      ]\n",
    "    },\n",
    "    \"entities\": [\n",
    "      \"default entity\"\n",
    "    ],\n",
    "    \"source\": \"cms-monit-notifier\",\n",
    "    \"status\": \"OK\",\n",
    "    \"targets\": [\n",
    "      \"email\"\n",
    "    ]\n",
    "  },\n",
    "  \"notification_endpoint\": \"http://monit-alarms.cern.ch:10011\"\n",
    "}'''\n",
    "                                    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile('notifier.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_flow = filter_alert_data.writeStream\\\n",
    ".foreach(lambda alert: notifier.send_notification(subject=alert.system,description=json.dumps(alert.asDict(), default=str)))\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alert_flow.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_flow.processAllAvailable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_flow.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_data_week_flow=raw_data.writeStream \\\n",
    ".outputMode(\"append\")\\\n",
    ".format(\"parquet\")\\\n",
    " .option(\"path\", \"/cms/users/carizapo/ming/moving_avg_cmsweb_logs\") \\\n",
    " .option(\"checkpointLocation\", \"/cms/users/carizapo/ming/checkpoint_moving_avg_cmsweb_logs\") \\\n",
    " .outputMode(\"append\") \\\n",
    " .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_data_week_flow.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7f04fb6b18d0>,\n",
       " <pyspark.sql.streaming.StreamingQuery at 0x7f04fb6b19e8>,\n",
       " <pyspark.sql.streaming.StreamingQuery at 0x7f04fb6b1a90>,\n",
       " <pyspark.sql.streaming.StreamingQuery at 0x7f04fb6b1908>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars.packages",
     "value": "org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1"
    },
    {
     "name": "spark.jars",
     "value": "/eos/home-y/ysunthor/SWAN_projects/StreamingNotebook"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
