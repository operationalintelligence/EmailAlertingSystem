{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user pykafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykafka import KafkaClient\n",
    "client = KafkaClient(hosts=\"188.185.79.229:9092\")\n",
    "topic = client.topics['email_alert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = topic.get_simple_consumer()\n",
    "for message in consumer:\n",
    "    print(message.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.3.1/spark-streaming-kafka-0-8-assembly_2.11-2.3.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming.kafka import TopicAndPartition\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.sendingAlert(subject, count)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sendingAlert(subject,count):\n",
    "    return [subject,count,count>10]\n",
    "spark.udf.register(\"sendingAlert\", sendingAlert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 5) #change frequency of getting data\n",
    "ssc.checkpoint(\"_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaStream =KafkaUtils.createDirectStream(ssc, [\"email_alert\"],\n",
    "                                           {\n",
    "                                               \"metadata.broker.list\":\"188.185.79.229:9092\",\n",
    "                                               \"auto.offset.reset\":\"smallest\"\n",
    "                                           }\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = kafkaStream.map(lambda v: json.loads(v[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-06-06 17:19:20\n",
      "-------------------------------------------\n",
      "['Cron <root@vocms041> cd /opt/CMSDataPopularity/xrootd; python2.6 lib/checkMViews.py', 32, True]\n",
      "['PopDB cronJob of cron_2019-04-05_04-30-06.log', 1, False]\n",
      "['WARNING: too many xrootd-cms-eos-popularity messages stored on the dashb brokers', 7, False]\n",
      "['OK: xrootd-cms-eos-popularity messages stored on the dashb brokers', 5, False]\n",
      "['PopDB cronJob of cron_2019-04-10_04-30-04.log', 1, False]\n",
      "['The collector xrootd_2 in vocms043.cern.ch was not running', 4, False]\n",
      "['Notification: Stop simplevisor at vocms043.cern.ch: 70%>=70%\\r\\n inodes', 12, True]\n",
      "['Notification: Stop two insertion agents at vocms042.cern.ch', 2, False]\n",
      "['PopDB cronJob of cron_2019-04-16_04-30-05.log', 1, False]\n",
      "['PopDB cronJob of cron_2019-04-23_04-30-03.log', 1, False]\n",
      "...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-06-06 17:19:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-06-06 17:19:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-06-06 17:19:35\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emails_dstream = parsed.map(lambda email: [email['subject'],email['timestamp'],1])\\\n",
    "                        .reduceByKeyAndWindow(lambda x,y: x+y, 10, 5)\n",
    "# emails_dstream.pprint()\n",
    "# benchmark=emails_dstream.map(lambda x:[x[0],x[1],x[1]>30])\n",
    "benchmark=emails_dstream.map(lambda x: sendingAlert(x[0],x[1]))\n",
    "# structuredOutput.foreachRDD(rdd => \n",
    "#   import sparkSession.implicits._\n",
    "#   val df = rdd.toDF()\n",
    "#   df.write.format(\"parquet\").mode(\"append\").save(s\"$workDir/$targetFile\")\n",
    "# })\n",
    "benchmark.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTerminationOrTimeout(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.sendingAlert(subject, count)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.map().reduceByKeyAndWindow(,, windowDuration=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_counts_sorted_dstream = subject_counts.transform(\\\n",
    "#   (lambda foo:foo\\\n",
    "#    .sortBy(lambda x:( -x[1]))))\n",
    "# top_five_subjects = subject_counts_sorted_dstream.transform\\\n",
    "#   (lambda rdd:sc.parallelize(rdd.take(5)))\n",
    "# top_five_subjects.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.0.0-preview/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars",
     "value": "/eos/home-y/ysunthor/SWAN_projects/test"
    },
    {
     "name": "spark.jars.packages",
     "value": "org.apache.spark:spark-streaming-kafka-0-8-assembly_2.11:2.3.1"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
