{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user pykafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykafka import KafkaClient\n",
    "client = KafkaClient(hosts=\"188.185.79.229:9092\")\n",
    "\n",
    "topic = client.topics['email_alert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = topic.get_simple_consumer()\n",
    "for message in consumer:\n",
    "    print(message.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.3.1/spark-streaming-kafka-0-8-assembly_2.11-2.3.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming.kafka import TopicAndPartition\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendingAlert(subject,count):\n",
    "    return [subject,count,count>10]\n",
    "spark.udf.register(\"sendingAlert\", sendingAlert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 5) #change frequency of getting data\n",
    "ssc.checkpoint(\"_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaStream =KafkaUtils.createDirectStream(ssc, [\"email_alert\"],\n",
    "                                           {\n",
    "                                               \"metadata.broker.list\":\"188.185.79.229:9092\",\n",
    "                                               \"auto.offset.reset\":\"smallest\"\n",
    "                                           }\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = kafkaStream.map(lambda v: json.loads(v[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emails_dstream = parsed.map(lambda email: [email['subject'],email['timestamp'],1])\\\n",
    "                        .reduceByKeyAndWindow(lambda x,y: x+y, 10, 5)\n",
    "# emails_dstream.pprint()\n",
    "# benchmark=emails_dstream.map(lambda x:[x[0],x[1],x[1]>30])\n",
    "benchmark=emails_dstream.map(lambda x: sendingAlert(x[0],x[1]))\n",
    "# structuredOutput.foreachRDD(rdd => \n",
    "#   import sparkSession.implicits._\n",
    "#   val df = rdd.toDF()\n",
    "#   df.write.format(\"parquet\").mode(\"append\").save(s\"$workDir/$targetFile\")\n",
    "# })\n",
    "benchmark.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTerminationOrTimeout(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "try:\n",
    "    print('Welcome to parse engine')\n",
    "    consumer = KafkaConsumer('cmsweb_logs',\n",
    "                             bootstrap_servers='monit-kafka.cern.ch:9092',\n",
    "#To subscribe since the first message that has been produced to kafka producer\n",
    "#                              auto_offset_reset='earliest',\n",
    "                             enable_auto_commit=False,\n",
    "                             group_id=None,\n",
    "                             value_deserializer=lambda m: json.loads(m)\n",
    "                            )\n",
    "    for message in consumer:\n",
    "        print(message)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # Logs the error appropriately.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars",
     "value": "/eos/home-y/ysunthor/SWAN_projects/StreamingNotebook"
    },
    {
     "name": "spark.jars.packages",
     "value": "org.apache.spark:spark-streaming-kafka-0-8-assembly_2.11:2.3.1"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
