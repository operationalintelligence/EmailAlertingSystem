{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo-list\n",
    "=========\n",
    "<li>Migrate static->stream(if possible)</li>\n",
    "<li>Find new efficient way to do pattern detection/anomaly detection</li>\n",
    "<li>Email alert config (now sent to yanisa.sunthornyotin@cern.ch)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "userSchema = StructType() \\\n",
    "        .add(\"window\",StructType()\\\n",
    "             .add(\"start\",TimestampType())\\\n",
    "             .add(\"end\",TimestampType()))\\\n",
    "        .add(\"system\", StringType())\\\n",
    "        .add(\"api\", StringType())\\\n",
    "        .add(\"user\", StringType())\\\n",
    "        .add(\"count_req\", LongType())\\\n",
    "        .add(\"req_load\", LongType())\\\n",
    "        .add(\"system_load\", LongType())\\\n",
    "        .add(\"api_load\", LongType())\\\n",
    "        .add(\"user_load\", LongType())\\\n",
    "        .add(\"avg_req\", DoubleType())\\\n",
    "        .add(\"%diff_req\", DoubleType())\\\n",
    "        .add(\"avg_sys\", DoubleType())\\\n",
    "        .add(\"%diff_sys\", DoubleType())\\\n",
    "        .add(\"avg_api\", DoubleType())\\\n",
    "        .add(\"%diff_api\", DoubleType())\\\n",
    "        .add(\"avg_user\", DoubleType())\\\n",
    "        .add(\"%diff_user\", DoubleType())\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For static flow to join\n",
    "raw_data = spark\\\n",
    ".readStream.format(\"parquet\")\\\n",
    ".schema(userSchema)\\\n",
    ".load(\"/cms/users/carizapo/ming/fullDiff_cmsweb_logs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- system: string (nullable = true)\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- api: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- count_req: long (nullable = true)\n",
      " |-- req_load: long (nullable = true)\n",
      " |-- system_load: long (nullable = false)\n",
      " |-- api_load: long (nullable = false)\n",
      " |-- user_load: long (nullable = false)\n",
      " |-- avg_req: double (nullable = true)\n",
      " |-- %diff_req: double (nullable = true)\n",
      " |-- avg_sys: double (nullable = true)\n",
      " |-- %diff_sys: double (nullable = true)\n",
      " |-- avg_api: double (nullable = true)\n",
      " |-- %diff_api: double (nullable = true)\n",
      " |-- avg_user: double (nullable = true)\n",
      " |-- %diff_user: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alerts_hist = spark\\\n",
    ".read.format(\"parquet\")\\\n",
    ".load(\"/cms/users/carizapo/ming/fullDiff_cmsweb_logs\");\n",
    "alerts_hist.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+----+---------+--------+-----------+--------+---------+-------+---------+-------+---------+-------+---------+--------+----------+\n",
      "|system|window|api|user|count_req|req_load|system_load|api_load|user_load|avg_req|%diff_req|avg_sys|%diff_sys|avg_api|%diff_api|avg_user|%diff_user|\n",
      "+------+------+---+----+---------+--------+-----------+--------+---------+-------+---------+-------+---------+-------+---------+--------+----------+\n",
      "+------+------+---+----+---------+--------+-----------+--------+---------+-------+---------+-------+---------+-------+---------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alerts_hist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col=['window']\n",
    "raw_data_init = alerts_hist.withColumn('date',col(\"window.start\")).drop(*drop_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: The input column system_hash should have at least two distinct values.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_95apython3/x86_64-centos7-gcc7-opt/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_95apython3/x86_64-centos7-gcc7-opt/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o740.fit.\n: java.lang.IllegalArgumentException: requirement failed: The input column system_hash should have at least two distinct values.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.feature.OneHotEncoderCommon$$anonfun$9.apply(OneHotEncoderEstimator.scala:456)\n\tat org.apache.spark.ml.feature.OneHotEncoderCommon$$anonfun$9.apply(OneHotEncoderEstimator.scala:454)\n\tat scala.Option.map(Option.scala:146)\n\tat org.apache.spark.ml.feature.OneHotEncoderCommon$.transformOutputColumnSchema(OneHotEncoderEstimator.scala:454)\n\tat org.apache.spark.ml.feature.OneHotEncoderBase$$anonfun$3.apply(OneHotEncoderEstimator.scala:89)\n\tat org.apache.spark.ml.feature.OneHotEncoderBase$$anonfun$3.apply(OneHotEncoderEstimator.scala:88)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.ml.feature.OneHotEncoderBase$class.validateAndTransformSchema(OneHotEncoderEstimator.scala:88)\n\tat org.apache.spark.ml.feature.OneHotEncoderEstimator.validateAndTransformSchema(OneHotEncoderEstimator.scala:120)\n\tat org.apache.spark.ml.feature.OneHotEncoderEstimator.transformSchema(OneHotEncoderEstimator.scala:145)\n\tat org.apache.spark.ml.feature.OneHotEncoderEstimator.fit(OneHotEncoderEstimator.scala:151)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-0185d028bae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys_indexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_indexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mapi_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpipelineModel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipelineModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_95apython3/x86_64-centos7-gcc7-opt/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_95apython3/x86_64-centos7-gcc7-opt/lib/python3.6/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_95apython3/x86_64-centos7-gcc7-opt/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_95apython3/x86_64-centos7-gcc7-opt/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_95apython3/x86_64-centos7-gcc7-opt/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_95apython3/x86_64-centos7-gcc7-opt/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_95apython3/x86_64-centos7-gcc7-opt/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: The input column system_hash should have at least two distinct values.'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n",
    "\n",
    "\n",
    "sys_indexer = StringIndexer(inputCol=\"system\", outputCol=\"system_hash\")\n",
    "user_indexer = StringIndexer(inputCol=\"user\", outputCol=\"user_hash\")\n",
    "api_indexer = StringIndexer(inputCol=\"api\", outputCol=\"api_hash\")\n",
    "inputs = [sys_indexer.getOutputCol(), user_indexer.getOutputCol(),api_indexer.getOutputCol()]\n",
    "encoder = OneHotEncoderEstimator(inputCols=inputs, outputCols=[\"system_vec\",\"user_vec\",\"api_vec\"])\n",
    "\n",
    "pipeline = Pipeline(stages=[sys_indexer,user_indexer,api_indexer, encoder])\n",
    "pipelineModel=pipeline.fit(raw_data_init)\n",
    "result=pipelineModel.transform(raw_data_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run StoreItemDemand/custom_transformers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = result.randomSplit([0.8,0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_data.withColumn('set', lit(0))\n",
    "df_train = df_train.withColumn('id', lit(-1))\n",
    "df_test = test_data.withColumn('set', lit(1))\n",
    "\n",
    "joined = df_test.union(df_train.select(*df_test.columns))\n",
    "\n",
    "train_data = joined.filter('set == 0')\n",
    "test_data = joined.filter('set == 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_data.randomSplit([0.8,0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler,StandardScaler\n",
    "# Feature extraction\n",
    "dc = DateConverter(inputCol='date', outputCol='dateFormated')\n",
    "hrex = HourExtractor(inputCol='date')\n",
    "minex = MinExtractor(inputCol='date')\n",
    "dex = DayExtractor(inputCol='dateFormated')\n",
    "mex = MonthExtractor(inputCol='dateFormated')\n",
    "yex = YearExtractor(inputCol='dateFormated')\n",
    "wdex = WeekDayExtractor(inputCol='dateFormated')\n",
    "wex = WeekendExtractor()\n",
    "mbex = MonthBeginExtractor()\n",
    "meex = MonthEndExtractor()\n",
    "# Data process\n",
    "va = VectorAssembler(inputCols=[\"system_vec\",\"user_vec\",\"api_vec\",'count_req','%diff_req','%diff_sys','%diff_api','%diff_user'\\\n",
    "                                 ,'weekday', 'weekend', 'monthbegin', 'monthend','hour','minute', 'day', 'month', 'year'], outputCol=\"features\")\n",
    "# scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "# scaler = MinMaxScaler(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[dc,hrex,minex, dex, mex,wdex,wex,mbex,meex, yex, va])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed = pipeline_model.transform(train)\n",
    "validation_transformed = pipeline_model.transform(validation)\n",
    "test_transformed = pipeline_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, we choose the k that start to make no change to y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model and train/predict the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "kmeans = KMeans(k=10, seed=1)  # 10 clusters here\n",
    "model = kmeans.fit(validation_transformed.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(train_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the outlier to spark dataframe\n",
    "<li>Add calculated distance column to predictions dataframe</li>\n",
    "<li>Drop unnecessary column from predictions dataframe</li>\n",
    "<li>Add Max_distance/Label column to dataframe</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(train_transformed)\n",
    "from scipy.spatial import distance\n",
    "columns_drop=['system_hash','user_hash','api_hash','system_vec','user_vec','api_vec','dateFormated','hour','minute'\\\n",
    "             ,'day','month','year','weekday','weekend','monthbegin','monthend','features']\n",
    "centers = model.clusterCenters()\n",
    "fixed_entry = centers #for example, the entry against which you want distances\n",
    "distance_udf = udf(lambda x,y: float(distance.euclidean(x, fixed_entry[y])), FloatType())\n",
    "\n",
    "# For joining static dataframe\n",
    "predictions = predictions.withColumn('distances', distance_udf(col('features'),col('prediction')))\\\n",
    ".select('*', avg('distances').over(Window.partitionBy('prediction')).alias('avg_distances'),\\\n",
    "       stddev('distances').over(Window.partitionBy('prediction')).alias('std_distances'),\\\n",
    "       max('distances').over(Window.partitionBy('prediction')).alias('max_distances'))\\\n",
    ".drop(*columns_drop)\n",
    "# predictions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Benchmark for detect outlier\n",
    "<li>Define outlier region to be +/- 10% of the max value</li>\n",
    "<li>Define the datapoint that have the distances further than 2 standard deviation of the mean distance per cluster as an outlier</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_factor=0.1\n",
    "# alert_udf= udf(lambda max_distances,dist: dist>= max_distances-(max_distances*outlier_factor), BooleanType())\n",
    "alert_udf= udf(lambda avg_dist,std_dist,dist: dist>= avg_dist+2*std_dist, BooleanType())\n",
    "#Static\n",
    "alerts = predictions.withColumn('label', alert_udf(col('avg_distances'),col('std_distances'),col('distances')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish new alert from Kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static processing\n",
    "alerts_broadcast= alerts.select([c for c in alerts.columns if c in\\\n",
    "                                 {'label','system','api','user','date'}]).where(alerts.label==1)\n",
    "alerts_broadcast=alerts_broadcast.toDF('system_temp', 'api_temp','user_temp','date','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alerts_email=raw_data.join(alerts_broadcast,[alerts_broadcast.date==raw_data.window.start,\\\n",
    "                                             alerts_broadcast.user_temp==raw_data.user,\\\n",
    "                                            alerts_broadcast.api_temp==raw_data.api,\\\n",
    "                                            alerts_broadcast.system_temp==raw_data.system] , \"inner\")\\\n",
    "                    .drop('system_temp','api_temp','user_temp','date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notifier import Notifier\n",
    "import json\n",
    "notifier = Notifier(config=json.loads(s='''\n",
    "{\n",
    "  \"cases\": {\n",
    "    \"exit_2\": {\n",
    "      \"alert_name\": \"cms-htcondor-es-validation\",\n",
    "      \"email\": {\n",
    "        \"send_ok\": true,\n",
    "        \"to\": [\n",
    "          \"yanisa.sunthornyotin@cern.ch\"\n",
    "        ]\n",
    "      },\n",
    "      \"entities\": [\n",
    "        \"default entity\"\n",
    "      ],\n",
    "      \"snow\": {\n",
    "        \"assignment_level\": 3,\n",
    "        \"functional_element\": \"\",\n",
    "        \"grouping\": true,\n",
    "        \"service_element\": \"MONITORING\"\n",
    "      },\n",
    "      \"source\": \"cms-monit-notifier\",\n",
    "      \"status\": \"ERROR\",\n",
    "      \"targets\": [\n",
    "        \"email\",\n",
    "        \"snow\"\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"default_case\": {\n",
    "    \"alert_name\": \"cms-htcondor-es-validation\",\n",
    "    \"email\": {\n",
    "      \"send_ok\": true,\n",
    "      \"to\": [\n",
    "        \"yanisa.sunthornyotin@cern.ch\"\n",
    "      ]\n",
    "    },\n",
    "    \"entities\": [\n",
    "      \"default entity\"\n",
    "    ],\n",
    "    \"source\": \"cms-monit-notifier\",\n",
    "    \"status\": \"OK\",\n",
    "    \"targets\": [\n",
    "      \"email\"\n",
    "    ]\n",
    "  },\n",
    "  \"notification_endpoint\": \"http://monit-alarms.cern.ch:10011\"\n",
    "}'''\n",
    "                                    ))\n",
    "sc.addPyFile('notifier.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Static processing\n",
    "alert_flow = alerts_email.writeStream\\\n",
    ".foreach(lambda alert: notifier.send_notification(subject=alert.system,description=json.dumps(alert.asDict(), default=str)))\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7fc9451ec9b0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_flow.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '208e9c58-cf5b-4cd9-8c90-399cedc1aa0a',\n",
       " 'runId': 'a0941b1e-88d6-48b8-b359-d2f23b466405',\n",
       " 'name': None,\n",
       " 'timestamp': '2019-08-05T11:55:33.224Z',\n",
       " 'batchId': 0,\n",
       " 'numInputRows': 2940,\n",
       " 'processedRowsPerSecond': 9.398736605201913,\n",
       " 'durationMs': {'addBatch': 309710,\n",
       "  'getBatch': 794,\n",
       "  'getOffset': 1475,\n",
       "  'queryPlanning': 702,\n",
       "  'triggerExecution': 312807,\n",
       "  'walCommit': 49},\n",
       " 'stateOperators': [{'numRowsTotal': 2988,\n",
       "   'numRowsUpdated': 2988,\n",
       "   'memoryUsedBytes': 3103463,\n",
       "   'customMetrics': {'loadedMapCacheHitCount': 0,\n",
       "    'loadedMapCacheMissCount': 0,\n",
       "    'stateOnCurrentVersionSizeBytes': 1931815}}],\n",
       " 'sources': [{'description': 'FileStreamSource[hdfs://analytix/cms/users/carizapo/ming/fullDiff_cmsweb_logs]',\n",
       "   'startOffset': None,\n",
       "   'endOffset': {'logOffset': 0},\n",
       "   'numInputRows': 2940,\n",
       "   'processedRowsPerSecond': 9.398736605201913}],\n",
       " 'sink': {'description': 'ForeachWriterProvider(org.apache.spark.sql.execution.python.PythonForeachWriter@47fe4140,Right(<function1>))'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alert_flow.lastProgress\n",
    "# alert_flow.processAllAvailable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans Model evaluation using Silhouette method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Elbow method to determine which 'K' to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.clustering import KMeans\n",
    "# from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# cost = np.zeros(20)\n",
    "# for k in range(2,20):\n",
    "#     kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "#     model = kmeans.fit(train_transformed.sample(False,0.1, seed=1))\n",
    "#     cost[k] = model.computeCost(train_transformed) # requires Spark 2.0 or later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "# ax.plot(range(2,20),cost[2:20])\n",
    "# ax.set_xlabel('k')\n",
    "# ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate clustering by computing Silhouette score\n",
    "# evaluator = ClusteringEvaluator()\n",
    "# silhouette = evaluator.evaluate(predictions)\n",
    "# print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a glimpse of how the training result looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_df=predictions.toPandas()\n",
    "# print(transform_df['system'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# filter_data=transform_df[(transform_df.system =='couchdb')]\n",
    "# filter_data.loc[:,'outlier']=filter_data.apply(lambda x:x['distances']>=x['avg_distances']+x['std_distances'],axis=1)\n",
    "# filter_data[filter_data.avg_distances>=0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_data[filter_data.outlier==True].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_data.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Euclidean distance and label the outlier\n",
    "\n",
    "Make the group of furthest point from cluster center to be the outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# max_df=(filter_data.groupby('prediction'))['distances'].max()\n",
    "# outlier_factor=0.4\n",
    "# filter_data.loc[:,'outlier']=filter_data.apply(lambda x:x['distances']>= max_df.loc[x['prediction']]-(max_df.loc[x['prediction']]*outlier_factor), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep=filter_data[['%diff_req','%diff_sys','%diff_api','%diff_user']]\n",
    "# label=filter_data[['outlier']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# prep_fix=prep.loc['2019-07-17 07:02:00':'2019-07-18 11:26:00']\n",
    "# label_fix=label.loc['2019-07-17 07:02:00':'2019-07-18 11:26:00']\n",
    "# prep_fix.plot()\n",
    "# plt.plot(prep_fix.index,label_fix.outlier,'o')\n",
    "# prep.plot()\n",
    "# plt.plot(prep.index,label.outlier,'o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.driver.memory",
     "value": "10g"
    },
    {
     "name": "spark.executor.instances",
     "value": "10"
    },
    {
     "name": "spark.executor.memory",
     "value": "10g"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
