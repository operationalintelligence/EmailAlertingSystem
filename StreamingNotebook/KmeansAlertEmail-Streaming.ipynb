{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo-list\n",
    "=========\n",
    "<li>Migrate static->stream(if possible)</li>\n",
    "<li>Find new efficient way to do pattern detection/anomaly detection</li>\n",
    "<li>Email alert config (now sent to yanisa.sunthornyotin@cern.ch)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "userSchema = StructType() \\\n",
    "        .add(\"window\",StructType()\\\n",
    "             .add(\"start\",TimestampType())\\\n",
    "             .add(\"end\",TimestampType()))\\\n",
    "        .add(\"system\", StringType())\\\n",
    "        .add(\"api\", StringType())\\\n",
    "        .add(\"user\", StringType())\\\n",
    "        .add(\"count_req\", LongType())\\\n",
    "        .add(\"req_load\", LongType())\\\n",
    "        .add(\"system_load\", LongType())\\\n",
    "        .add(\"api_load\", LongType())\\\n",
    "        .add(\"user_load\", LongType())\\\n",
    "        .add(\"avg_req\", DoubleType())\\\n",
    "        .add(\"%diff_req\", DoubleType())\\\n",
    "        .add(\"avg_sys\", DoubleType())\\\n",
    "        .add(\"%diff_sys\", DoubleType())\\\n",
    "        .add(\"avg_api\", DoubleType())\\\n",
    "        .add(\"%diff_api\", DoubleType())\\\n",
    "        .add(\"avg_user\", DoubleType())\\\n",
    "        .add(\"%diff_user\", DoubleType())\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For analyze real-time\n",
    "alerts = spark\\\n",
    ".readStream.format(\"parquet\")\\\n",
    ".schema(userSchema)\\\n",
    ".load(\"/cms/users/carizapo/ming/fullDiff_cmsweb_logs\");\n",
    "# alerts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alerts_hist = spark\\\n",
    ".read.format(\"parquet\")\\\n",
    ".load(\"/cms/users/carizapo/ming/fullDiff_cmsweb_logs\");\n",
    "# alerts_hist.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+---------+--------+-----------+--------+---------+-------+---------+-------+---------+-------+---------+--------+----------+\n",
      "|    system|              window|                 api|                user|count_req|req_load|system_load|api_load|user_load|avg_req|%diff_req|avg_sys|%diff_sys|avg_api|%diff_api|avg_user|%diff_user|\n",
      "+----------+--------------------+--------------------+--------------------+---------+--------+-----------+--------+---------+-------+---------+-------+---------+-------+---------+--------+----------+\n",
      "|crabserver|[2019-08-05 14:38...|                info|/DC=ch/DC=cern/OU...|        2|       2|       1435|     112|       15|    2.0|      0.0| 1435.0|      0.0|  112.0|      0.0|    15.0|       0.0|\n",
      "|crabserver|[2019-08-05 14:38...|                task|/DC=ch/DC=cern/OU...|        2|       2|       1435|     119|       15|    2.0|      0.0| 1435.0|      0.0|  119.0|      0.0|    15.0|       0.0|\n",
      "|crabserver|[2019-08-05 14:28...|   fileusertransfers|/DC=ch/DC=cern/OU...|        8|      28|       1435|     401|       15|   28.0|      0.0| 1435.0|      0.0|  401.0|      0.0|    15.0|       0.0|\n",
      "|crabserver|[2019-08-05 14:38...|   fileusertransfers|/DC=ch/DC=cern/OU...|       10|      28|       1435|     401|       15|   28.0|      0.0| 1435.0|      0.0|  401.0|      0.0|    15.0|       0.0|\n",
      "|crabserver|[2019-08-05 14:29...|   fileusertransfers|/DC=ch/DC=cern/OU...|        4|      28|       1435|     401|       15|   28.0|      0.0| 1435.0|      0.0|  401.0|      0.0|    15.0|       0.0|\n",
      "|crabserver|[2019-08-05 14:42...|   fileusertransfers|/DC=ch/DC=cern/OU...|        6|      28|       1435|     401|       15|   28.0|      0.0| 1435.0|      0.0|  401.0|      0.0|    15.0|       0.0|\n",
      "|    phedex|[2019-08-05 14:29...|             lfn2pfn|/DC=ch/DC=cern/OU...|        2|       4|        592|     307|       15|    4.0|      0.0|  592.0|      0.0|  307.0|      0.0|    15.0|       0.0|\n",
      "|    phedex|[2019-08-05 14:28...|             lfn2pfn|/DC=ch/DC=cern/OU...|        2|       4|        592|     307|       15|    4.0|      0.0|  592.0|      0.0|  307.0|      0.0|    15.0|       0.0|\n",
      "| scheddmon|[2019-08-05 14:38...|        status_cache|/DC=ch/DC=cern/OU...|        1|       1|         79|      78|       15|    1.0|      0.0|   79.0|      0.0|   78.0|      0.0|    15.0|       0.0|\n",
      "|crabserver|[2019-08-05 14:29...|       filetransfers|/DC=ch/DC=cern/OU...|        2|       4|       1435|     356|       15|    4.0|      0.0| 1435.0|      0.0|  356.0|      0.0|    15.0|       0.0|\n",
      "|crabserver|[2019-08-05 14:28...|       filetransfers|/DC=ch/DC=cern/OU...|        2|       4|       1435|     356|       15|    4.0|      0.0| 1435.0|      0.0|  356.0|      0.0|    15.0|       0.0|\n",
      "|crabserver|[2019-08-05 14:38...|        filemetadata|/DC=ch/DC=cern/OU...|        6|      24|       1435|     373|       15|   24.0|      0.0| 1435.0|      0.0|  373.0|      0.0|    15.0|       0.0|\n",
      "|crabserver|[2019-08-05 14:29...|        filemetadata|/DC=ch/DC=cern/OU...|        7|      24|       1435|     373|       15|   24.0|      0.0| 1435.0|      0.0|  373.0|      0.0|    15.0|       0.0|\n",
      "|crabserver|[2019-08-05 14:28...|        filemetadata|/DC=ch/DC=cern/OU...|        3|      24|       1435|     373|       15|   24.0|      0.0| 1435.0|      0.0|  373.0|      0.0|    15.0|       0.0|\n",
      "|crabserver|[2019-08-05 14:23...|        filemetadata|/DC=ch/DC=cern/OU...|        8|      24|       1435|     373|       15|   24.0|      0.0| 1435.0|      0.0|  373.0|      0.0|    15.0|       0.0|\n",
      "|   couchdb|[2019-08-05 14:27...|asikdar_RVCMSSW_1...|/DC=ch/DC=cern/OU...|        1|       8|      19391|       6|    16027|    8.0|      0.0|19391.0|      0.0|    6.0|      0.0| 16027.0|       0.0|\n",
      "|   couchdb|[2019-08-05 14:23...|asikdar_RVCMSSW_1...|/DC=ch/DC=cern/OU...|        1|       8|      19391|       6|    16027|    8.0|      0.0|19391.0|      0.0|    6.0|      0.0| 16027.0|       0.0|\n",
      "|   couchdb|[2019-08-05 14:28...|asikdar_RVCMSSW_1...|/DC=ch/DC=cern/OU...|        1|       8|      19391|       6|    16027|    8.0|      0.0|19391.0|      0.0|    6.0|      0.0| 16027.0|       0.0|\n",
      "|   couchdb|[2019-08-05 14:29...|asikdar_RVCMSSW_1...|/DC=ch/DC=cern/OU...|        3|       8|      19391|       6|    16027|    8.0|      0.0|19391.0|      0.0|    6.0|      0.0| 16027.0|       0.0|\n",
      "|   couchdb|[2019-08-05 14:38...|asikdar_RVCMSSW_1...|/DC=ch/DC=cern/OU...|        2|       8|      19391|       6|    16027|    8.0|      0.0|19391.0|      0.0|    6.0|      0.0| 16027.0|       0.0|\n",
      "+----------+--------------------+--------------------+--------------------+---------+--------+-----------+--------+---------+-------+---------+-------+---------+-------+---------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alerts_hist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col=['window']\n",
    "raw_data_init = alerts_hist.withColumn('date',col(\"window.start\")).drop(*drop_col)\n",
    "stream_data = alerts.withColumn('date',col(\"window.start\")).drop(*drop_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n",
    "\n",
    "\n",
    "sys_indexer = StringIndexer(inputCol=\"system\", outputCol=\"system_hash\")\n",
    "user_indexer = StringIndexer(inputCol=\"user\", outputCol=\"user_hash\")\n",
    "api_indexer = StringIndexer(inputCol=\"api\", outputCol=\"api_hash\")\n",
    "inputs = [sys_indexer.getOutputCol(), user_indexer.getOutputCol(),api_indexer.getOutputCol()]\n",
    "encoder = OneHotEncoderEstimator(inputCols=inputs, outputCols=[\"system_vec\",\"user_vec\",\"api_vec\"])\n",
    "\n",
    "pipeline = Pipeline(stages=[sys_indexer,user_indexer,api_indexer, encoder])\n",
    "pipelineModel=pipeline.fit(raw_data_init)\n",
    "result=pipelineModel.transform(raw_data_init)\n",
    "stream_result=pipelineModel.transform(stream_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run StoreItemDemand/custom_transformers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = result.randomSplit([0.8,0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_data.withColumn('set', lit(0))\n",
    "df_train = df_train.withColumn('id', lit(-1))\n",
    "df_test = test_data.withColumn('set', lit(1))\n",
    "\n",
    "joined = df_test.union(df_train.select(*df_test.columns))\n",
    "\n",
    "train_data = joined.filter('set == 0')\n",
    "test_data = joined.filter('set == 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_data.randomSplit([0.8,0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler,StandardScaler\n",
    "# Feature extraction\n",
    "dc = DateConverter(inputCol='date', outputCol='dateFormated')\n",
    "hrex = HourExtractor(inputCol='date')\n",
    "minex = MinExtractor(inputCol='date')\n",
    "dex = DayExtractor(inputCol='dateFormated')\n",
    "mex = MonthExtractor(inputCol='dateFormated')\n",
    "yex = YearExtractor(inputCol='dateFormated')\n",
    "wdex = WeekDayExtractor(inputCol='dateFormated')\n",
    "wex = WeekendExtractor()\n",
    "mbex = MonthBeginExtractor()\n",
    "meex = MonthEndExtractor()\n",
    "# Data process\n",
    "va = VectorAssembler(inputCols=[\"system_vec\",\"user_vec\",\"api_vec\",'count_req','%diff_req','%diff_sys','%diff_api','%diff_user'\\\n",
    "                                 ,'weekday', 'weekend', 'monthbegin', 'monthend','hour','minute', 'day', 'month', 'year'], outputCol=\"features\")\n",
    "# scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "# scaler = MinMaxScaler(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[dc,hrex,minex, dex, mex,wdex,wex,mbex,meex, yex, va])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed = pipeline_model.transform(train)\n",
    "validation_transformed = pipeline_model.transform(validation)\n",
    "test_transformed = pipeline_model.transform(test_data)\n",
    "stream_transformed = pipeline_model.transform(stream_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, we choose the k that start to make no change to y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model and train/predict the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "kmeans = KMeans(k=10, seed=1)  # 10 clusters here\n",
    "model = kmeans.fit(validation_transformed.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(train_transformed)\n",
    "stream_prediction=model.transform(stream_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the outlier to spark dataframe\n",
    "<li>Add calculated distance column to predictions dataframe</li>\n",
    "<li>Drop unnecessary column from predictions dataframe</li>\n",
    "<li>Add Max_distance/Label column to dataframe</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prediction: integer (nullable = false)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- avg_distances: double (nullable = true)\n",
      " |-- std_distances: double (nullable = true)\n",
      " |-- max_distances: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- prediction: integer (nullable = false)\n",
      " |-- system: string (nullable = true)\n",
      " |-- api: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- count_req: long (nullable = true)\n",
      " |-- req_load: long (nullable = true)\n",
      " |-- system_load: long (nullable = true)\n",
      " |-- api_load: long (nullable = true)\n",
      " |-- user_load: long (nullable = true)\n",
      " |-- avg_req: double (nullable = true)\n",
      " |-- %diff_req: double (nullable = true)\n",
      " |-- avg_sys: double (nullable = true)\n",
      " |-- %diff_sys: double (nullable = true)\n",
      " |-- avg_api: double (nullable = true)\n",
      " |-- %diff_api: double (nullable = true)\n",
      " |-- avg_user: double (nullable = true)\n",
      " |-- %diff_user: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- distances: float (nullable = true)\n",
      " |-- avg_distances: double (nullable = true)\n",
      " |-- std_distances: double (nullable = true)\n",
      " |-- max_distances: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(train_transformed)\n",
    "stream_prediction=model.transform(stream_transformed)\n",
    "from scipy.spatial import distance\n",
    "columns_drop=['system_hash','user_hash','api_hash','system_vec','user_vec','api_vec','dateFormated','hour','minute'\\\n",
    "             ,'day','month','year','weekday','weekend','monthbegin','monthend','features']\n",
    "centers = model.clusterCenters()\n",
    "fixed_entry = centers #for example, the entry against which you want distances\n",
    "distance_udf = udf(lambda x,y: float(distance.euclidean(x, fixed_entry[y])), FloatType())\n",
    "\n",
    "# For joining streaming dataframe\n",
    "stream_prediction = stream_prediction.withColumn('distances', distance_udf(col('features'),col('prediction'))).drop(*columns_drop)\n",
    "distances_benchmark = stream_prediction\\\n",
    ".withWatermark(\"date\", \"1 minute\")\\\n",
    ".groupby('prediction',window('date', \"1 minute\", \"1 minute\"))\\\n",
    ".agg(avg('distances').alias('avg_distances'),stddev('distances').alias('std_distances'),max('distances').alias('max_distances'))\n",
    "\n",
    "# max_dist_df = predictions.groupBy(\"prediction\")\\\n",
    "# .agg(max(\"distances\").alias(\"max_dist\"))\n",
    "# predictions=predictions.join(max_dist_df, [\"prediction\"], \"inner\")\n",
    "stream_predictions = stream_prediction.join(distances_benchmark,\"prediction\",\"inner\").drop(\"window\")\n",
    "\n",
    "distances_benchmark.printSchema()\n",
    "stream_predictions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Benchmark for detect outlier\n",
    "<li>Define outlier region to be +/- 10% of the max value</li>\n",
    "<li>Define the datapoint that have the distances further than 2 standard deviation of the mean distance per cluster as an outlier</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_factor=0.1\n",
    "# alert_udf= udf(lambda max_distances,dist: dist>= max_distances-(max_distances*outlier_factor), BooleanType())\n",
    "alert_udf= udf(lambda avg_dist,std_dist,dist: dist>= avg_dist+2*std_dist, BooleanType())\n",
    "stream_alerts = stream_predictions.withColumn('label', alert_udf(col('avg_distances'),col('std_distances'),col('distances')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish new alert from Kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_alerts_broadcast= stream_alerts.select([c for c in stream_alerts.columns if c in\\\n",
    "                                 {'label','system','api','user','date'}]).where(stream_alerts.label==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notifier import Notifier\n",
    "import json\n",
    "notifier = Notifier(config=json.loads(s='''\n",
    "{\n",
    "  \"cases\": {\n",
    "    \"exit_2\": {\n",
    "      \"alert_name\": \"cms-htcondor-es-validation\",\n",
    "      \"email\": {\n",
    "        \"send_ok\": true,\n",
    "        \"to\": [\n",
    "          \"yanisa.sunthornyotin@cern.ch\"\n",
    "        ]\n",
    "      },\n",
    "      \"entities\": [\n",
    "        \"default entity\"\n",
    "      ],\n",
    "      \"snow\": {\n",
    "        \"assignment_level\": 3,\n",
    "        \"functional_element\": \"\",\n",
    "        \"grouping\": true,\n",
    "        \"service_element\": \"MONITORING\"\n",
    "      },\n",
    "      \"source\": \"cms-monit-notifier\",\n",
    "      \"status\": \"ERROR\",\n",
    "      \"targets\": [\n",
    "        \"email\",\n",
    "        \"snow\"\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"default_case\": {\n",
    "    \"alert_name\": \"cms-htcondor-es-validation\",\n",
    "    \"email\": {\n",
    "      \"send_ok\": true,\n",
    "      \"to\": [\n",
    "        \"yanisa.sunthornyotin@cern.ch\"\n",
    "      ]\n",
    "    },\n",
    "    \"entities\": [\n",
    "      \"default entity\"\n",
    "    ],\n",
    "    \"source\": \"cms-monit-notifier\",\n",
    "    \"status\": \"OK\",\n",
    "    \"targets\": [\n",
    "      \"email\"\n",
    "    ]\n",
    "  },\n",
    "  \"notification_endpoint\": \"http://monit-alarms.cern.ch:10011\"\n",
    "}'''\n",
    "                                    ))\n",
    "sc.addPyFile('notifier.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_streaming_flow = stream_alerts_broadcast.writeStream\\\n",
    ".foreach(lambda alert: notifier.send_notification(subject=alert.system,description=json.dumps(alert.asDict(), default=str)))\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_diff_flow = stream_alerts_broadcast.writeStream.queryName(\"MLdf\").outputMode(\"Append\").format(\"memory\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7f87212813c8>,\n",
       " <pyspark.sql.streaming.StreamingQuery at 0x7f8721281358>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_diff_flow.lastProgress\n",
    "alert_streaming_flow.processAllAvailable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = spark.sql(\"select * from MLdf\")\n",
    "query.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_diff_flow.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_streaming_flow.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans Model evaluation using Silhouette method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Elbow method to determine which 'K' to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.clustering import KMeans\n",
    "# from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# cost = np.zeros(20)\n",
    "# for k in range(2,20):\n",
    "#     kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "#     model = kmeans.fit(train_transformed.sample(False,0.1, seed=1))\n",
    "#     cost[k] = model.computeCost(train_transformed) # requires Spark 2.0 or later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "# ax.plot(range(2,20),cost[2:20])\n",
    "# ax.set_xlabel('k')\n",
    "# ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate clustering by computing Silhouette score\n",
    "# evaluator = ClusteringEvaluator()\n",
    "# silhouette = evaluator.evaluate(predictions)\n",
    "# print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a glimpse of how the training result looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_df=predictions.toPandas()\n",
    "# print(transform_df['system'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# filter_data=transform_df[(transform_df.system =='couchdb')]\n",
    "# filter_data.loc[:,'outlier']=filter_data.apply(lambda x:x['distances']>=x['avg_distances']+x['std_distances'],axis=1)\n",
    "# filter_data[filter_data.avg_distances>=0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_data[filter_data.outlier==True].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_data.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Euclidean distance and label the outlier\n",
    "\n",
    "Make the group of furthest point from cluster center to be the outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# max_df=(filter_data.groupby('prediction'))['distances'].max()\n",
    "# outlier_factor=0.4\n",
    "# filter_data.loc[:,'outlier']=filter_data.apply(lambda x:x['distances']>= max_df.loc[x['prediction']]-(max_df.loc[x['prediction']]*outlier_factor), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep=filter_data[['%diff_req','%diff_sys','%diff_api','%diff_user']]\n",
    "# label=filter_data[['outlier']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# prep_fix=prep.loc['2019-07-17 07:02:00':'2019-07-18 11:26:00']\n",
    "# label_fix=label.loc['2019-07-17 07:02:00':'2019-07-18 11:26:00']\n",
    "# prep_fix.plot()\n",
    "# plt.plot(prep_fix.index,label_fix.outlier,'o')\n",
    "# prep.plot()\n",
    "# plt.plot(prep.index,label.outlier,'o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.driver.memory",
     "value": "10g"
    },
    {
     "name": "spark.executor.instances",
     "value": "10"
    },
    {
     "name": "spark.executor.memory",
     "value": "10g"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
